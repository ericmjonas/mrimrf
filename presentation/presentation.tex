\documentclass{beamer}
\usetheme{Warsaw}

\mode<presentation>

\title{Bayesian Phase Unwrapping with Factor Graphs}
\author{Eric Jonas}
\date{May 12, 2009}
\institute[6.556]{MIT Department of Brain and Cognitive Sciences}


\begin{document}

\begin{frame}
\maketitle
\end{frame}

\section{Phase Unwrapping for MR}
\begin{frame}

\end{frame}

\section{Factor Graphs and Markov Random Fields}

\begin{frame}
\frametitle{Markov Random Fields and Factor Graphs}

Markov random field, undirected graphical model, etc. 

Factor Graph: a particular language / notation for markov random fields

Show CPT
talk about markov blanket

\cite{Kschischang01}
\end{frame}

\begin{frame} 
\frametitle{Factor Graphs for Low-Level Vision}
Properties of image MRFs
large number of verticies
O(1) (constant local) connectivity
\end{frame}

\begin{frame}
\frametitle{Bayesian Factor Graphs}
Decompose into prior, likelihood, etc. 
\end{frame}

\subsection{MRFs for Phase Unwrapping}

\begin{frame} 
\frametitle{MRFs for Phase: Frey's approach}
more classical image MRF with delta functions, continuous state
\end{frame}

\begin{frame}
\frametitle{discrete latent state, uniform factors}
\end{frame}

\begin{frame}
\frametitle{discrete latent state, unique factors}
My formulation
\end{frame} 

\section{Inference in MRFs}

\begin{frame}
\frametitle{Inference in MRFs}
Our MRF has given us $p*(x | D)$, which is not convex, 
and not even a valid probability distribution. 

We would like to somehow ``solve'' this system to get a rough
sense of the distribution $p(x |D)$. 

Two generic approaches: 
\begin{itemize}
\item draw samples from $p(x | D)$ to empirically estimate 
\item optimize to find MAP solution
\end{itemize}
\pause

We focus on sampling. 
\end{frame}

\begin{frame}
  \frametitle{Markov-Chain Monte Carlo}
  Markov Property: next state only depends on current state

  \begin{equation}
    p(x_{t+1} | x_{1:t} = p(x_{t+1} | x_{t})
  \end{equation}

  Ergodic markov chains have stationary distributions
  
  Set up a state space so that the expectation is the target distribution
  Used in situations where you know $\pi^\ast(x)$ but not $\pi(x)$. 
\end{frame}

\begin{frame}
  \frametitle{Metropolis Hastings}
  One way to construct this Markov Chain
  
  \begin{equation}
    a = \min (1, \frac{p(x^\ast)}{p(x)} \cdot 
  \frac{q(x \rightarrow x^\ast)}{q(x^\ast \rightarrow x)})
  \end{equation}

  %FIXME MH walk figure
  \cite{Metropolis1953}
\end{frame}

\begin{frame}
  \frametitle{Gibbs Sampling}
  like MH but along an axis, useful when we can condition
  on other variables. 
  Look, we can gibbs sample in image MRFs with discrete state spaces
  \cite{GemanGeman1983}
  % Fixme Gibbs s
\end{frame}

\begin{frame}
  \frametitle{Tempering} 
  Like Simulated Annealing
\end{frame}

\begin{frame}
  \frametitle{Swendsen-Wang}
  Work Through
\end{frame}

\begin{frame}
  \frametitle{MRFs and Parallelism}
  The conditional independence assumptions allow fine-grained parallelism
  
\end{frame}

\section{Our Implementation}
\begin{frame}
  \frametitle{Our Implementation}
  use SW, etc. 
  python, numpy, scipy, c++, boost, etc. 
  multithreaded
\end{frame}

\section{Performance}

How to measure performance? I'm going to go for 
log-likelihood, 


\subsection{Synthetic Data}
\begin{frame}
  \frametitle{2-D Synthetic Data}
\end{frame}

\begin{frame}
  \frametitle{3-D Synthetic Data}
\end{frame}

\subsection{Phantoms}

\subsection{Actual Data}
\begin{frame}
  \frametitle{Div and Audrey}
\end{frame}

\section{Methods Comparison}
\begin{frame}
  \frametitle{PRELUDE}
\end{frame}

\section{Concluson and Future Directions}
\begin{frame}
  \frametitle{Where to now?}
  Exact sampling using Systematic Stochsatic Search
  Better neighborhood connectivity / likelihood? 
  GPU implementation 
  Better visualization of posterior?
\end{frame}

\begin{frame}
  \frametitle{More information}
  Source is on github
  
\end{frame}

\end{document}
